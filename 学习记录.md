uv init
1. 激活虚拟环境
.venv\Scripts\activate.bat
uv sync
可以配置代理，加快下载速度
另外也测试了docker部署环境 win上自带NVIDIA Container Toolkit Linux要自己下载
再下载pytouch+cuda+cudnn的镜像
在镜像基础上配自己要的环境
学习的话可以启动jupter lab
训练的话可以跑一次性的 跑完就关
--rm 参数会在容器退出后自动删除

用所有词创建映射词典，增加特殊符号，总有一些词映射不到
bpe 把长词分开
文本->单词->utf->unicode->bpe分词->encode>decode->byte decode->utf
bpe还用了lru缓存

环境建议的 num_workers
Windows + Jupyter	0 (必须)
Windows + .py 文件	0 或小值 (1-2)
Linux	可以用 4-8
softmax取e 全部变为正数 差异更明显
和交叉熵合用 求导是本身 训练时， 一类变小softmax使其他类就变大了
实现多头注意力把qkv进行了拆分 然后换顺序 矩阵乘法计算后再换回来还原 

FlashAttention 最佳性能  分块计算 + 在线Softmax + IO优化
context = F.scaled_dot_product_attention(
    queries, keys, values, is_causal=True
)


StreamingLLM  部分有，可以直接用 不要拿来训练
保留前4个 + 最后1020个
研究发现：Attention 分布极度不均匀
Token Position:  [BOS] [t2] [t3] ... [t1000] [t1001]
Attention 权重:  [45%] [2%] [1%] ... [0.1%]  [50%]
                  ↑                            ↑
              起始token                    最近token
              吸收大量注意力              实际相关内容


Grouped-Query Attention
几个q共享kv 加kv缓存 大大降低参数量和计算量 

Multi-Head Latent Attention
将kv chche压缩存储 节省内存 还能当正则化用

Sliding Window Attention
部分块限制了上下文长度  记忆能力差

MOE专家 替换ffn 每次只激活部分专家

deltanet 引入lstm的思想？

```python
class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x
```
微调把最后的输出头改了 分类就改成类的数量
LoRA是低秩矩阵

学习率预热 余弦退火 梯度裁剪
